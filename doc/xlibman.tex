\documentclass[11pt,preprint]{aastex}
\input{define.tex}
\usepackage[hmargin=2.0cm,vmargin=2.0cm]{geometry}
\geometry{letterpaper}
\citestyle{aa}

%\slugcomment{\scriptsize Last update: {\mmddyyyydate\today}}

\begin{document}

\title{CASA Pipeline for the CARMA/STING \hato\ Data Reduction}
\author{Rui Xue\altaffilmark{1}}
\altaffiltext{1}{Department of Astronomy, University of Illinois, 
Urbana-Champaign, IL 61801, USA}


\begin{comment}

The purpose: the whole process is repeatedable. And any small adjuestement can be implemented quickly. Process a large sample in a uniform ways. Parameter setup are splitted out.

The above 3-step CASA scripts were written with the general purpose of VLA HI data reduction in mind, and will serve as an alternative solution for building an ancillary HI dataset to supplement the CARMA (STING) program. Those scripts require no major changes for different science targets and instrumental setups. They can handle the combination of datasets from different VLA observational programs.

A separated CASA script (see below), including required/optional parameters and working flow statements, will employ the above three scripts for a complete data reduction process. Example scripts are in the Section\,\ref{examples}.

To get it working:
coding highlight
shell > casapy --nologger --log2term
casapy > execfile('n4254.inp')

wrap everything into a parameter files..


Desgin: including most of working flows, automatically select the best parameters for reductions,
reduction the interactions and make it ideals tool for a large amount of data.


%		sometimes, you may want to do a timebin to reduce the data size
%		split(vis='XX',outputvis='XX.new',datacolumn='data',timebin='10s')
%		flagmanager(vis = msfile,mode='save',versionname='Original',comment='Original flags at import into CASA',\
%			merge='replace')

Keep all paramates in one places.
Keep all outputs figures together
Keep Log 
s
The submodule in xutils.py is more handy, rather than the pipeline scripts

Bad: 

Easy to generate science ready quality data products

2 Codes Structures
	input files: or dictionary…
	flow scrpts files: xload.py/xcal.py/xmerge.py/xclean.py
	useful modules: useful even not using pipelines

\end{comment}


\section{Overview}
...
\section{Examples}
...
\section{Caveats}

\begin{enumerate}

\item {\sc concat()} will merge spectral windows from different correlations into
a single spectral window if they have the same channel setup.
However, {\sc clean()} would expect a spectral window ID always has the same input polarization(s), so the second polarization setup of each spw it encounters will be ignored. In addition, if you have RR/LL dual polarization data and only RR or LL was flagged, {\sc clean()} will ignore both of them. 
To avoid merging spws with different correlation setups or throwing away all polarizations even just one goes bad, one can keep individual polarization under different spws by splitting and regridding them into slightly different channel setups.
 
\item Note that TOPO refers to a time stamp at a given observation date. If more than one obervation is concatenated this may lead to vastly erroneous values. Any conversion from TOPO to other frames such as BARY and LSRK should be performed for each individual observation, prior to concatenation or simultaneous imaging.

\end{enumerate}

\begin{comment}
\subsection{CASA vs. MIRAID}

\begin{enumerate}

\item The imaging weights and uncertainties of visibility amplitude are stored in the weight and sigma columns. 
However, in MIRIAD, the weighting images are calculated on-the-fly from $T_{\rm sys}$ and gain tables.

\item Subtracting \clean components in the visibility domain has some advantages in accuracy (like what the Clark CLEAN does during the major cycle). 
However, fft can be expensive for for large visibility datasets. 
The tradition \clean method is  the Hobgom \clean, which uses the dirty image and beam for source subtraction and doesn't go back to the visibility data.

\end{enumerate}


\subsection{Multi-Scale {\sc CLEAN}}
\end{comment}
\begin{comment}
\begin{enumerate}

\item The muti-scale clean is sensitivity to the visibility error (which usually shown as large-scale artificial structures). 
So there are some circumstances that it won't work.

\item The cookbook has some suggestions of scale selections: 0,1,3,5xbeams

\item the selection of largest scale. prior knowledges, largest angular scale can be traced defined from the visibility distribution
you have native component or not negcomponnet=1

\end{enumerate}

\end{comment}


\label{examples}

\begin{comment}
Information needed in your master script:

Observation Description: 
The NRAO VLA archive provides the basic observation information for all raw files: Obs Date/Log, Array Configuration, Correlator Setting, Source/Fluxcal/Phasecal/Passcal, etc. And the step1 script will also generate a *.listobs.log file after data importing. It includes a comprehensive description of the dataset and almost everything you need to set the reduction parameters. 
Required/Optional Parameters: 
Please check the header of each reduction script for explanations of all required or optional parameters. 
\end{comment}


\begin{comment}
\subsection{Single-Track}

\subsubsection{BIMA data}
\subsubsection{CARMA data}
\subsubsection{HI}
\subsubsection{CONTINUE}

\subsection{Multiple Tracks}

\subsubsection{CAMRA}
\subsubsection{BIMA+CARMA}
\subsubsection{VLA+EVLA}
\end{comment}

\begin{comment}

   VLA CALIBRATORS INFORMATION
%#
%#       http://www.vla.nrao.edu/astro/calib/manual/csource.html 
%# 		http://www.vla.nrao.edu/astro/calib/search/

\section{To-Do}

1) automatically select beams
2) automatically clean files
3) virtual concat and concat?
4) if your original wt is okay. Rewt will not change the beam much for singletrack..
5) Diagnostic figures

Relative wt between two tracks are the same…
Beam size the same
Noise level the same 

<Jorsater \& van Moorsel 1995>
<THINGS paper>
<HERICALES paper>

clean init.py files

mask0 is slow

feather testing

There are several other flagging tasks in CASA but looks like eventually only flagcmd and tflagdata will be left (see new cookbook p155) or http://www.aoc.nrao.edu/~rurvashi/FlaggerDocs/FlaggerDocs.html


Now when you’re merging several tracks, a virtual ms file will be created which links the tracks visibility. This will save your space.  Your final multi-track merging MS file will also be much smaller (by a factor of 2-3)

And also a new parameter <wt\_scale> can scale the weighting of each track MS (<concat>->visweightscale). 

visweightscale -- The weights of the individual MSs will be scaled in the concatenated
        output MS by the factors in this list. Useful for handling heterogeneous arrays.
        Use plotms to inspect the "Wt" column as a reference for determining the scaling 
        factors. See the cookbook for more details.
        example: [1.,3.,3.] - scale the weights of the second and third MS by a factor 3.
        default: [] (empty list) - no scaling

So do not remove the *src.ms of each tracks before doing multi-track combined CLEAN.
In the past, I was using the toolkit level to do the same thing, but this new feature is much more handy.


More on wt\_stat!!!

Move splitting ->hanning spectral/top-bary regridding->wt\_stat to imerge.py and

Then we could image the calibrators

Clean will not save model visiisbity to MS when cleaning done. This will save some disk space and reduce the I/O load, and speed up the script (significantly from some of my tests). But still, if your the memory needed for cleaning is larger than the physical memeory of your machine, the virtual swap memory will be hit up and the process will be still slow. So reduce the size of  your output CLEAN image  as small as possible. This could be done by enlarge the pixel size (to a still reasonable level) and shink the image pixel number. Check the visstta log will give you a hint by checking the predicted sythesisze beam size. This will become X2X3 smaller file size. Image Model are still saved (not in vibislity format but stay in image, much saveing space compared with vis)

Have a check at the wighting:
Plotms(yaxis=’wt’,)

Both of the below two are in imerge.py
new task statwt to derive visibility data weights based on their scatter 
concat: >visweightscale -- The weights of the individual MSs will be scaled in the concatenated

or plotms to check overall visibility…
 
Other minor change:
	flagversion ‘original’ will be checked at the beginning of the calibration script. If it doesn’t exist, then we will save a flagversion called ‘Original’. This will help some cases if your working flow start from an MS data was not generated from my data-loading script.



uvspec for solving slow issues using averaging
uvplt

Plotting scripts….
Asdm?
Image the calibrator
CleanUp!

Using viwer of pyfits to generate some quick check images. Aply fits


Complicated…Moments maps


Cleaning / override -> input MS

output MS by the factors in this list. Useful for handling heterogeneous arrays.
        Use plotms to inspect the "Wt" column as a reference for determining the scaling 
        factors. See the cookbook for more details.
        example: [1.,3.,3.] - scale the weights of the second and third MS by a factor 3.
        default: [] (empty list) - no scaling

flagged data are not copied when splitting reduce files size!
Avoid any unnesscary create correed or model columns
This will save a disk space of flagged/total*dataset
And to unify weight scale, you shoud…
Regrid to common spectral window… Calculate the weight together…

Choosing line free channel, otherwise your’re have low weight on signal visibility

DO NOT USE PDF for native uvuslity plotting

PLOTxy will give large file when using pdf or eps, but small low-resolution file in png
PLOTMS file is smaller 

use wt\_stat if you belive that weights is crazy now (for JVLA data)
for VLA data, using wt\_stat although scaling the overall to a different level, but relative wt is more or less the same. So doesn’t make much effects for single-track imaging.
--
Many tasks (e.g. clean) are insensitive to an overall
    scale error in WEIGHT, but are affected by errors in the relative weights
    between visibilities.

Note: casa shipped its own pylab/matplot.pyfit etc.. Also you can imported your own modeuls from outside word

When splitting source, keep the spw tag???
--
test for VLA data plot two wt difference
test for scaling wt
--

use wt\_scale if you belive that multi tracks weights are not scaled corrected.

How myriad calculate wt? Robust weighting


For JVLA+VLA data imaging. Turn on wt\_stat for each tracks. And check
Combined visility weights. Then if they are not at the same scale, use wt\_scale to scaling them.
check PLOT->WT

*  
X2-X4 smaller

Multi clean crashing issues solved..

Absoluted path older in prefix….

plotms: UVW coordinates can now be displayed in units of wavelengths (enabling the full uv-coverage view of multi-frequency syntesis data)
*  

new task asdmsummary displays a summary of the content of an ASDM. 

*  new task plotweather to display the weather conditions. plotweather also calculates the atmospheric opacities for each spw using actual and historical weather data for the EVLA

•	pyfits is now included in CASA
•	alphy fits from my multimedia program
•	
*  improved feather task, including new parameters for weighting data, single dish amplitude scaling, and plotting
*  
concat can now apply additional, user supplied weights to the input data sets /// concat virtyllu /// testconcat


automatically choose pixel size<<=


model images!!
No concat???

\end{comment}

\section{Reference}


\appendix



\section{Files Structures}

\begin{itemize}

\item ximport.py

import/inspect/prepare visibility from uvfits/miriad/asdm/ms

\item xcal.py

calibrations: baseline correction / gain calibration / flux calibration

\item xcalplot.py

plotting calibration diagnostic figures

\item xconsol.py

extract calibrated source data / continuum subtraction / concatenate data from multiple tracks / adjust visibility weight

\item xclean.py

imaging/{\sc CLEAN}ing spectral line / continuum data

\item xutils.py

functions supporting the pipeline

\item init.py

casapy initialization file for the pipeline setup

\item xinit.py

pipeline parameter initialization

\item xexp.py

experimental functions for testing


\end{itemize}

\begin{comment}
\section{Basic Principle- How to Write Calibration Scripts?}
			During observations, the astronomical signals collected by telescopes go through 
			the signal processing system (digitized, amplified, etc.), and are finally converted 
			into the so-called 
			"Data". In this process, 
			many instrument effects and artificial features will be added into data. 
			The purpose of
			calibrations is by following the signal processing path <strong>backward</strong> 
			, to correct the instrumental effects introduced at each stage, and to restore 
			original signals and astronomical information. 
			So before writing calibration pipelines, one should make a sketch 
			on the path which signals go through, and
			identify all of the instrument processing effects. Then you can write modules
			for each corrections and finalizing your calibration pipeline.
 			It must be notted that the sequence of different corrections does matter in most cases.
\section{Modulization / Structure / Working Flow}

			The calibration pipeline usually include steps for different 	
			corrections and a complete 		
			working flow. The modular programming and strcutured scripting technique 	
			will be helpful to reduce the 
			the complexity and increase the flexibility of the pipeline. 
			An example is, using a shell script to control the data reduction flow and 
			different calibration correction modules. Sometimes we only want to
			run a part of the calibration pipeline. Then modular and structured programming
			may give some benefits. Beyond the "historical famous" <a href='http://en.wikipedia.org/wiki/Goto'>goto
			</a> statement, 
			setting some optional boolean parameters to control the executions of different working modules 
			is an alternative way to achieve the goal. 
			
\section{comments}

			Although calibration scripts are usually only for personal usage, it is still
			a good habit to embed some comments into them. They may include the explaination for each
			input paramater, the naming convention for the generated files, or some marks for working flow.  
			It will not only help other people to reuse your scripts, but also benefit yourself 
			during the debugging and reviewing. 
			Providing documentation may be an alternative way, but for professional astronomers (usually amateur 
			programers..) some comments embeded in scripts are enough.

\section{Parameters Setting}		

			The input information for calibrations includes not only the
			"data", but also "parameters" to describe the observations, 
			instrument setup, calibration tuning methods, etc. 
			Some "parameters" may be hidden 
			into the dataset (e.g. FITS file head information) or Astronomical Packages you are using
			(e.g. like the antenna position correction file in MIRIAD), the others may 
			require you to write into the calibration pipeline. It is an useful strategy to
			separate such input parameters from your calibration scripts and leave them 
			into a "input file". Then when
			you change them, you don't need to dig into the calibration scripts. 
			This is a convient way to
			make your script useable for different observations. 
			In addition, calibration script themselves
			can provide some default values for optional parameters, which make 
			the parameter input file concise.
			Another approach to reduce the number of the required parameters, is making 
			use of some library routines or package tasks to read them from 
			the dataset directly if existing there..	
			
\section{Intermediate Files / Naming Conventions}
			The calibration corrections will produce different version of data at eacha stage. 
			If your calibrations are time consuming 
			and the disk space saving is not your priority, 
			then keeping those files for the later detailed calibration
			examination is better than discarding them. A good naming conventions can help a lot 
			for distinguishing
			data at different steps. 			

			On the other hand, you may try the calibration pipeline 
			several times by tuning input parameters, and compare different final productions. Then 
			signing each production with unique tags in file names, will make the examinations easier and won't mess your
			data folder. Such tags can also be used for initial input files and summary files associated with each try
			
			%In my experience, a nice naming convention may be ('obs_id'.'reduc_tag'.'reduc_stage'.***.'data_kind').
			Remember to include an explanation for the naming convention. It's easy to forget them and make
			one confusing later.
			
\section{Cross-Platform}
			In most cases, data calibrations are performed in 
			various *nix systems. Usually a scripting language is portable enough for
			the pipeline working well on different platforms. But paying cautions on the
			platform-independent part and potential different syntax doesn't cost you much time. 
			If scripts use some 3rd party 
			libraries or routines, you'd better set their path as optional parameters in the scripts.

\section{Flagging}
			A clever script may be able to do the whole data calibration job automatically. But unfortunately
			an iterative flagging are often required. You can set 
			an option for iterative flagging or automatic flagging, and save satisfied flagged data files. 
			Then in the next 
			try, you can start with those flagged files and don't do the same job again.   
			
\section{changelog}

2012-06-19

* use the new flagcmd(). supposedly faster, manual flagging parameter flagselect='' 
  follows the new syntax of flagcmd().
* source visibility splitting has been moved to imerge\_2012****.py
* reduce file sizes of calibrated source visibilities by a factor of 2~3, by using 
  some new features in v3.4
* add the function of recalculating sigma\&weight in imerge\_2012****.py by setting 
  wtstat=True
    e.g. wtstat=True
         wtstat\_fitspw="1:7~17,0:46~56"
* when merging multiple tracks using imerge\_2012***.py, you could scale weights of 
  individual track by a assigned factor (e.g. wt\_scale=[7,1,1])

-------
About weighting, we have several choices for some adjustments:
1) we could using statwt() to recalculate weights/sigma for individual tracks from 
   the visibility scatter.
2) we could scale the weights of each track when concatenating multi-track visibilities.
3) we could choose briggs-robust or natural imaging weighting.
4) we could also use tapering.
...
All of them have been implemented in the reduction scripts. I guess they are enough to 
deal with some isssues when combing VLA\& JVLA data (if knowing how to use the above 
methods correctly....)

For method1, I did several tests using old VLA data with existing meanful weights/sigma
column recorded by the system. If choosing right line-free channels to estimate the vis 
scatter, the sigma/weights derived by statwt() do represent the relative weights 
of each baseline, though overall rescaled by some factor (see the figures in 
test/statwt\_fitchan ). The resulted new synthesized beam and cube noise characteristic 
are more or less the same as those using original weights. 
This method may be useful for data from the JVLA and the VLA->JVLA transition year, 
because of the absence of correct weights in original dataset.

method2 is the first thing to try if you suspect the relative overall weight scaling 
among different tracks doesn't match or if you want more dramatic changing to enhance SNR. 
The mismatching scaling often happens when combing tracks observed at different "ages" of 
VLA \& JVLA.

method3/4 is more like small tuning. And it DOESN'T really downweight data with a wrong 
relative overall weight scaling!

-------
Note:
* please copy init.py into ~/.casa/ and modify some parameters according to your setting.
* a set of reduction examples is in scripts/sting-hi/n4254/*inp
* multi-scale cleaning doesn't randomly crash casapy in mac10.7 anymore......
* feather() has been tested and it works just like immerge. a new script will wrap it up.
* I am collecting the code to generate calibration/imaging diagnostic plots to a new 
script. 


\section{Reference}

  http://casa.nrao.edu  CASA Homepage  
  http://casa.nrao.edu/Doc/Cookbook/casa\_cookbook.pdf  CASA cookbook  
  http://casaguides.nrao.edu/  CASA Wiki  
  http://casa.nrao.edu/active/docs/casaref/CasaRef.html  Toolkit Reference Manual  
  http://casa.nrao.edu/active/docs/taskref/TaskRef.html  Task Reference Manual  
  http://casa.nrao.edu/release\_FAQ.shtml  CASA Known Issues  
  https://safe.nrao.edu/wiki/bin/view/Software/CASA-AIPSDictionary  AIPS -- CASA dictionary  
  https://safe.nrao.edu/wiki/bin/view/Software/CASA-MIRIADDictionary  MIRIAD -- CASA dictionary  
  http://aipsforum.blogspot.com/  Collected Shreds of AIPS Wisdom  
  http://listmgr.cv.nrao.edu/pipermail/daip  AIPS support Maillist  
  http://www.vla.nrao.edu/astro/calib/manual/csource.html  VLA Calibrator Manual  
  http://www.vla.nrao.edu/astro/  VLA/EVLA Information for Astronomers  
  http://www.vla.nrao.edu/astro/guides/evlareturn/aliasing.shtml/  Aliasing Problem  
  http://www.mpia-hd.mpg.de/THINGS/Overview.html  The HI Nearby Galaxy Survey  
  http://www.vla.nrao.edu/astro/  VLA Information  
  http://science.nrao.edu/evla/  EVLA Information  
  http://www.iram.fr/IRAMFR/IS/school.htm  IRAM Interferometry Summer School  
  http://www.aoc.nrao.edu/events/synthesis/2010/  NRAO Synthesis Imaging Workshop  
  http://www.vla.nrao.edu/astro/guides/evlareturn/  VLA-EVLA Transition Information  
  http://www.vla.nrao.edu/astro/archive/baselines/  VLA-EVLA Baseline Correction  
  
\section{Useful Commands}

casapy --help

\section{Change Log}

\section{Single-Dish Reduction}

\end{comment}

\section{Functions in {\sc XUTILS}}

\subsection{{\sc xutils.scalewt()}}

The theoretical variance of each visibility record can be calculated as follows:
\begin{equation}
\sigma_{ij}^2={{C_{ij}^2 T_i T_j} \over {2 \Delta \nu \Delta T}},
\label{eq:viswt}
\end{equation}
in which, 
\begin{equation}
C_{ij}
=\frac{2 k_{\rm B}}{ \sqrt{(\eta_{i} A_i)(\eta_{j} A_j)} } \frac{1}{\eta_q}.
\end{equation}
$A$ and $\eta$ are the antenna collecting area and aperture efficiency, respectively. 
$\eta_q$ is the quantum efficiency from the backend \citep{Koda:2011fk}.
$\Delta \nu$ and $\Delta T$ are the channel width and integration time.
To optimize the imaging quality, each visibility record will be assigned with a weight during the Fourier transform.
In natural weighting, the weight is set to the inverse variance,
\begin{equation}
{\rm wt}_{ij}=\frac{1}{\sigma_{ij}^2}.
\label{eq:visna}
\end{equation}
For Briggs and uniform weighting\footnote{\url{http://casa.nrao.edu/docs/casaref/imager.weight.html}}, the weight are manipulated based on the basic visibility weight above and a gridding weight $W_{ij}$ (proportional to the UV sampling density function).

A MIRIAD dataset includes the baseline-dependent variable {\sc jyperk} (different from {\sc jyperka}), defined as
\begin{equation}
{\rm Jyperk}=\frac{2 k_{\rm B}}{ \sqrt{(\eta_{i} A_i)(\eta_{j} A_j)} }.
\end{equation}
Following Equation\,\ref{eq:visna}, the variance and visibility weight are calculated using the $T_{\rm sys}$ and Jyperk variables on-the-fly when they are requested by a task (e.g. {\sc INVERT}, {\sc FITS}, implemented in {\sc uvoi.c}). 
However, ${\eta_q}$ will be assumed to be 1. If a gain table exists, a modified Jyperk value will be used for the calculation (Jyperk$^{'}$=$g_ig_j\times$Jyperk and $g$ is the antenna-based gain amplitude here).


In CASA, the visibility weight is specified in the weight column of a measurement set (MS). 
The values are adjusted during calibrations with {\sc calwt=True} using gain and $T_{\rm sys}$ tables when available,
and the final weight in the calibrated MS are used as the basis for imaging.
It essentially works as the same way how MIRAID/{\sc INVERT} folds gains and $T_{\rm sys}$  into visibility weights with {\sc options=systemp}, but the current WEIGHT column implementation is handled correctly only in the relative sense:

\begin{itemize} 

\item VLA archival data have the initial weight column when importing them into a MS, and the values are filled following Equation\,\ref{eq:viswt} using the bandwidth of each spectral window (spw). Defining the WEIGHT column as the bandwidth weight rather than channel weight is confusing for spectral line imaging, 
because the current spw regridding tasks (e.g. cvel(), mstransform()) don't modify the weight column when the channel width changes. The WEIGHT\_SPECTRUM column (channel weight) is supposed to fill the blank, but it has not been implemented yet in most of calibration/imaging tasks.

\item Raw EVLA data still have initial weight values of 1 for all records when filled into a MS. The new switch power gain and $T_{\rm sys}$ derived from MS SYSPOWER/CALDEVICE subtables (using {\sc gencal}) can scale the weight values by,
\begin{equation}
{\rm wt}= \frac{1}{T_iT_j}.
\end{equation}
But they are only available in recent EVLA data and still may have some drawbacks if not carefully handled. For example, bad $T_{\rm sys}$ and swpow gain readings might create odd calibrated visibility and weight values.
In addition, scaling weights only using $T_{\rm sys}$ will not work when imaging a combined dataset with different backends / integrating time / channel width.
\end{itemize}

Although the above issues generally don't affect imaging individual observations, it might cause troubles when imaging a heterogenous dataset (e.g. VLA+EVLA) because the weight from different arrays could have incompatible absolute standards. To optimize the visibility weight of a heterogeneous dataset from different systems and observation setups, rescaling the weight is neccsary in CASA.

{\sc xutils.scalewt()} will evaluate the statistical results of the line-free data noise derived from {\sc statwt()}, and rescale both WEIGHT and SIGMA columns, with the SIGMA values redefined as the estimated RMS noise for single channel and WEIGHT=1/SIGMA$^2$. 
The scaling approach avoids the uncertainty of noise estimations for individual record when using {\sc statwt()}, by keeping the relative weight pattern in the original data. 
In addition, it doesn't depend on an accurate values of all coefficients required to calculate the absolute variance.

Figure\,\ref{fig:wtuvdist} shows the results of {\sc xutils.scalewt()} from three VLA/EVLA  \hone\ 21cm observations (one 1999 VLA B-config track in black, one 2000 VLA BC-config track in red, and one 2013 EVLA B-config observation in orange) towards NGC\,772.
Before rescaling, the weight values in the calibrated EVLA data (without applying the switch power table) is smaller by a factor of $>1000$ compared with the VLA data, causing the EVLA data incorrectly down-weighted. 
Considering differences in channel widths and integration intervals, the expected weight in the absolute sense should follow:
\begin{equation}
{\rm wt}=1/\sigma^2 \propto {{\Delta \nu \Delta T} \over (T_{\rm sys}/{\eta})^2},
 \end{equation}
Assuming $T_{\rm sys}/{\eta}$ decrease from 75K to 60K from VLA to EVLA in the L-band \citep{Ott:2008jx}, the expected weight ratio between these VLA and EVLA tracks will be $\sim$10, according to the observation details summarized in Table\,\ref{tab:wttest}.
The results from {\sc xutils.scalewt()} roughly matches this expectation. 

\begin{deluxetable}{ccccccc}
\tabletypesize{}
\tablecolumns{21} 
\tablecaption{Weight Rescaling Results for NGC0772 \label{tab:wttest}}
\tablewidth{0pt} 
\tablehead{
\multicolumn{1}{c}{Year}&
\multicolumn{1}{c}{Array}&
\multicolumn{1}{c}{Config}&
\multicolumn{1}{c}{ChanWidth (KHz)}&
\multicolumn{1}{c}{Pol.}&
\multicolumn{1}{c}{Integ.Interval (s)}&
\multicolumn{1}{c}{Weight (median)}
} 
\startdata
1999 & VLA    & D     & 48.83  & RR & 60 & 13.7 \\
2000 & VLA    & BC   & 97.66  & RR & 30 & 14.5 \\
2013 & EVLA  & BC   & 62.50  & RR LL & 3 & 1.2 \\
\enddata
\end{deluxetable}

\begin{figure}[hb]
\centering
\includegraphics[scale=.5]{figs/n0772hitest_wt_uvdist.png}
 \caption{WEIGHT values vs. the uv-distance for three tracks of NGC\,772. (this is from a dataset with rebinned spw, out-of-date)
 }
\label{fig:wtuvdist}
\end{figure}

\subsection{{\sc xutils.importmir() \& xutils.importmiriad()}}

xutils.importmir() will fill the MIRAID data into a MS using the MIRIAD-uvfits-MS approach. 
It requires several MIRAID tasks, and is basically a Python wrapper for the whole data filling process.
Alternatively, xutils.importmiriad() uses {\sc casafiller} (with several limitations and issues?).

\subsection{\sc xutils.xmoments()}

xutils.xmoments() is intended to replace the moments making algorithms we have in {\sc IDL}\footnote{\url{http://bitbucket.org/rxue/idl_moments}}.
Currently, only the 2D/3D-smooth+masking method is implemented.

\subsection{\sc xutils.resmoothpsf()}

Get the smoothed psf when using CLEAN() with option {\sc resmooth=True}. 
In this case, the smoothed psf is the psf in the CLEANed image.

\subsection{others}

\subsection{\sc xutils.xplotcal()}
\subsection{\sc xutils.flagtsys()}

Flag bad $T_{\rm sys}$ the EVLA switch power table using median filtering and clipping.

\subsection{\sc xutils.copyweight()}

copying weight between the WEIGHT and WEIGHT\_SPECTRUM columns

\bibliographystyle{apj}
\bibliography{papers2}

\end{document}  